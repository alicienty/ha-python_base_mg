# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kwuK_5wJ139Ba-tBMA5j8sbZU8YOJU44

Задание 1: Базовый загрузчик и анализатор данных

В этом задании мы напишем класс, который можно использовать для:
- Экспериментов с данными - валидации содержания датасетов
- Быстрой и удобной загрузки данных в программу для обработки big data
- Прототипирования ML моделей - подготовки данных для исследований
- Мониторинга данных - проверки качества входных данных в сложном пайплайне

Условие задачи:

Напишите класс DataLoader, который отвечает за загрузку и базовый анализ датасета.

1. Класс должен иметь один инкапсулированный (приватный) метод _validate_file_path, который принимает на вход путь к файлу, проверяет его существование и то, что это файл (а не директория). Если проверка не пройдена, метод должен вызывать исключение FileNotFoundError.

2. Реализуйте метод экземпляра load_data, который принимает путь к CSV-файлу, вызывает приватный метод _validate_file_path для проверки и затем загружает данные с помощью pandas.read_csv. Метод должен возвращать DataFrame.

3. Реализуйте второй метод экземпляра get_basic_info, который не принимает аргументов (кроме self), но использует данные, загруженные методом load_data. Этот метод должен выводить на экран (или возвращать в виде строки) базовую информацию о датасете: форму (shape), имена колонок и количество пропущенных значений в каждой колонке.

Критерии оценки (10 баллов):

3 балла: Корректная реализация инкапсулированного метода _validate_file_path с проверкой существования и типа пути.
3 балла: Корректная работа метода load_data, который использует приватный метод и возвращает DataFrame.
2 балла: Реализация метода get_basic_info, который выводит требуемую информацию о данных.
2 балла: Читаемость кода, наличие комментариев (docstrings) и отсутствие ошибок при выполнении.
"
"""

import os
import pandas as pd
import numpy as np

class DataLoader: #отвечает за загрузку и базовый анализ датасета

  def __init__(self, file_path):
    self.file_path = file_path #инициализируем класс
    self.data = self.load_data(file_path)

  def _validate_file_path(self, file_path): #проверяем корректность пути и является ли файл файлом
    if os.path.exists(file_path) == False:
      raise FileNotFoundError(f"Такого файла не существует")
    if os.path.isfile(file_path) == False:
      raise FileNotFoundError(f"Это не файл")

  def load_data(self, file_path): #загружаем данные
    self._validate_file_path(file_path) #используем приватный метод
    df = pd.read_csv(file_path)
    return df #возвращаем табличные данные

  def get_basic_info(self): #пишем инфо о нашем датасете
    df = self.load_data(self.file_path) #загружаем данные через прошлый метод
    arr = np.array(df)
    structure = arr.shape #получаем представление о структуре
    length = structure[0] #записываем количество строк
    span = structure[1] #и количество столбцов
    nulls = int(df.isnull().sum().sum()) #считаем общее количество нулей в датасете
    colmns = ", ".join(list(df.columns)) #составляем список колонок
    nulls_clmns = {} #делаем словарь для подсчета нулей в каждой колонке
    nulls_per_column = df.isnull().sum().to_dict()  # получаем словарь, где ключ — это колонка, а значение — количество пустых переменных

    return (f"Датасет имеет {length} строк/и и {span} столбца/ов. В таблице в общей сложности {nulls} пустых значений/я.\nДатасет имеет следующие колонки: {colmns}. В свою очередь колонки имеют следующее распределение пустых значений: {nulls_per_column}")